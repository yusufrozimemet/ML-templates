{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRkr0PvhZEAtvGE9ltfOd4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/deep_learning/AutoEncoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step Name                                              | Description                                                                                                 |\n",
        "|--------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n",
        "| Downloading the Dataset                               | This step involves downloading the MovieLens datasets (ml-100k and ml-1m) from the provided URLs and extracting them. |\n",
        "| Importing the Libraries                               | The necessary libraries (NumPy, Pandas, Torch) are imported for data manipulation and building the AutoEncoder neural network. |\n",
        "| Importing the Dataset                                 | The MovieLens datasets (movies, users, ratings) are imported using Pandas, but these datasets won't be used in this specific code. |\n",
        "| Preparing the Training Set and Test Set               | The training and test sets are loaded from the files \"u1.base\" and \"u1.test\" in the ml-100k dataset, respectively. |\n",
        "| Getting the Number of Users and Movies                | The total number of users and movies are calculated to determine the dimensions of the user-movie interaction matrix. |\n",
        "| Converting the Data into an Array with Users in Lines and Movies in Columns | The training and test sets are converted into a list of lists where each list corresponds to a user's ratings for all movies. |\n",
        "| Converting the Data into Torch Tensors                | The user-movie interaction matrices (training and test sets) are converted into Torch tensors, which are used in PyTorch for building the AutoEncoder neural network. |\n",
        "| Creating the Architecture of the Neural Network (AutoEncoder) | A class SAE is defined, which represents the AutoEncoder. It consists of fully connected layers to encode and decode the user-movie interaction matrix. |\n",
        "| Training the AutoEncoder                              | The AutoEncoder model is trained using Mean Squared Error (MSE) loss and RMSprop optimizer on the training set. |\n",
        "| Testing the AutoEncoder                               | The trained AutoEncoder model is tested using the test set to evaluate its performance. |\n"
      ],
      "metadata": {
        "id": "5fBsP7-bjOFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dataset\n",
        "!wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "!unzip ml-100k.zip\n",
        "!ls\n",
        "\n",
        "!wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
        "!unzip ml-1m.zip\n",
        "!ls\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Importing the dataset\n",
        "# We won't be using this dataset.\n",
        "movies = pd.read_csv('ml-1m/movies.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "users = pd.read_csv('ml-1m/users.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "\n",
        "# Preparing the training set and the test set\n",
        "training_set = pd.read_csv('ml-100k/u1.base', delimiter='\\t')\n",
        "training_set = np.array(training_set, dtype='int')\n",
        "test_set = pd.read_csv('ml-100k/u1.test', delimiter='\\t')\n",
        "test_set = np.array(test_set, dtype='int')\n",
        "\n",
        "# Getting the number of users and movies\n",
        "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
        "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))\n",
        "\n",
        "# Converting the data into an array with users in lines and movies in columns\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(1, nb_users + 1):\n",
        "        id_movies = data[:, 1][data[:, 0] == id_users]\n",
        "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "# Converting the data into Torch tensors\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)\n",
        "\n",
        "# Creating the architecture of the Neural Network (AutoEncoder)\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)\n",
        "\n",
        "# Training the SAE\n",
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = sae(input)\n",
        "            target.requires_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.data * mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step()\n",
        "    print('epoch: ' + str(epoch) + ' loss: ' + str(train_loss / s))\n",
        "\n",
        "# Testing the SAE\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input)\n",
        "        target.requires_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.data * mean_corrector)\n",
        "        s += 1.\n",
        "print('test loss: ' + str(test_loss / s))\n"
      ],
      "metadata": {
        "id": "4llK0EaMherw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}