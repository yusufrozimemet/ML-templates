{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzbaqn/bxdgaQtu82LL4+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/dimensionality_reduction/principal_component_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                                              | Explanation                                                                                                                     |\n",
        "|---------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|\n",
        "| 1. Splitting the dataset into the Training set and Test set | We divide the original dataset into two parts: the training set and the test set. The training set is used to train the model, and the test set is used to evaluate the model's performance on unseen data. |\n",
        "| 2. Feature Scaling                                | Feature scaling is the process of standardizing the range of independent variables (features) in the dataset. It ensures that all features have similar scales and prevents certain features from dominating the learning algorithm. We use StandardScaler to perform feature scaling, transforming the data to have a mean of 0 and a standard deviation of 1. |\n",
        "| 3. Applying PCA                                   | PCA (Principal Component Analysis) is a dimensionality reduction technique that transforms the dataset into a new coordinate system represented by principal components. It aims to capture the maximum variance in the data in the new dimensions while reducing the number of features. We specify the desired number of components (in this case, 2) to which we want to reduce the data. |\n",
        "| 4. Training the Logistic Regression model        | After applying PCA and obtaining the reduced feature vectors in the training set, we train a Logistic Regression model on this transformed data. Logistic Regression is a classification algorithm used for binary or multiclass classification tasks. It learns to separate the data into different classes based on the input features. |\n",
        "| 5. Making the Confusion Matrix                    | The Confusion Matrix is a table used to evaluate the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. It helps assess how well the model predicts the classes on the test set. |\n",
        "| 6. Visualising the Test set results (Decision boundary) | For 2D or 3D data after PCA, we can visualize the test set results by plotting the data points and the decision boundary learned by the Logistic Regression model. The decision boundary is the dividing line that separates the different classes in the transformed feature space. It gives us insight into how well the model has separated the data points into their respective classes. |\n",
        "\n"
      ],
      "metadata": {
        "id": "5fBsP7-bjOFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already split your dataset into X_train, X_test, y_train, and y_test\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Applying PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Training the Logistic Regression model on the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Making the Confusion Matrix (Not shown in the provided code)\n",
        "# To make predictions and evaluate the model's performance on the test set\n",
        "\n",
        "# Visualising the Test set results (Not shown in the provided code)\n",
        "# To visualize how the model separates the data into classes after PCA\n"
      ],
      "metadata": {
        "id": "P0sak9Pno7dS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}