{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPpFnkH0aaCDWg2Lnbo3oI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/dimensionality_reduction/linear_discriminant_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                                              | Explanation                                                                                                                     |\n",
        "|---------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|\n",
        "| 1. Applying LDA                                   | Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique that transforms the feature space into a lower-dimensional space while maximizing the class separability. It aims to find linear combinations of features that best discriminate between different classes. In this step, we specify the desired number of linear discriminants, which is set to 2 in this case (LD1 and LD2). |\n",
        "| 2. Training the Logistic Regression model        | After applying LDA and obtaining the transformed feature vectors in the training set, we train a Logistic Regression model on this reduced data. Logistic Regression is a popular classification algorithm used for binary or multiclass classification tasks. It learns to separate the data into different classes based on the input features. |\n",
        "| 3. Making the Confusion Matrix                    | The Confusion Matrix is a table used to evaluate the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. It helps assess how well the model predicts the classes on the test set. |\n",
        "| 4. Visualising the Test set results (Decision boundary) | To visualize the decision boundary of the Logistic Regression model, we create a grid of points spanning the range of LD1 and LD2 in the transformed feature space. We then apply the trained classifier to each point on the grid to predict the class and create a contour plot to visualize the decision boundary. This allows us to observe how well the model separates the data points into their respective classes. |\n"
      ],
      "metadata": {
        "id": "5fBsP7-bjOFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already split your dataset into X_train, X_test, y_train, and y_test\n",
        "\n",
        "# Applying LDA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "lda = LDA(n_components=2)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "# Training the Logistic Regression model on the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Making the Confusion Matrix (Not shown in the provided code)\n",
        "# To make predictions and evaluate the model's performance on the test set\n",
        "\n",
        "# Visualising the Test set results\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_test, y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n",
        "                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha=0.75, cmap=ListedColormap(('red', 'green', 'blue')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c=ListedColormap(('red', 'green', 'blue'))(i), label=j)\n",
        "plt.title('Logistic Regression (Test set)')\n",
        "plt.xlabel('LD1')\n",
        "plt.ylabel('LD2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "P0sak9Pno7dS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}