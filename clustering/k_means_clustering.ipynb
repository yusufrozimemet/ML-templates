{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFLo46tbNUh2SBDy6Toppx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/clustering/k_means_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                                              | Explanation                                                                                                                     |\n",
        "|---------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|\n",
        "| 1. Importing necessary libraries                 | We import the required libraries, including NumPy, Matplotlib, and KMeans from scikit-learn, for performing the clustering task. |\n",
        "| 2. The Elbow Method for finding the optimal number of clusters | We use the Elbow Method to determine the optimal number of clusters for K-Means. The Elbow Method helps us identify the point where adding more clusters does not significantly decrease the Within-Cluster Sum of Squares (WCSS). We plot the WCSS against the number of clusters and look for the \"elbow\" point on the plot. |\n",
        "| 3. Training the K-Means model                    | After determining the optimal number of clusters from the Elbow Method, we create a KMeans instance with that number of clusters and train it on the dataset using the fit method. We initialize the centroids using the 'k-means++' method for better convergence. |\n",
        "| 4. Visualizing the clusters                       | Finally, we plot the clusters and their centroids on a 2D scatter plot to visualize how the data points have been grouped into different clusters. Each cluster is represented with a different color, and the centroids are marked with yellow color. |\n"
      ],
      "metadata": {
        "id": "5fBsP7-bjOFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV6w-dMhjKch"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming you have your data stored in a variable 'X'\n",
        "\n",
        "# Using the elbow method to find the optimal number of clusters\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plotting the elbow method graph\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')  # Within-cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "# Training the K-Means model on the dataset with the chosen number of clusters\n",
        "kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualizing the clusters\n",
        "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s=100, c='red', label='Cluster 1')\n",
        "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=100, c='blue', label='Cluster 2')\n",
        "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s=100, c='green', label='Cluster 3')\n",
        "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s=100, c='cyan', label='Cluster 4')\n",
        "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s=100, c='magenta', label='Cluster 5')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label='Centroids')\n",
        "plt.title('Clusters of customers')\n",
        "plt.xlabel('Annual Income (k$)')\n",
        "plt.ylabel('Spending Score (1-100)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}