{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbkwuPvyuBAP8q/i206VKw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/clustering/hierarchical_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                                              | Explanation                                                                                                                     |\n",
        "|---------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|\n",
        "| 1. Importing necessary libraries                 | We import the required libraries, including NumPy, Matplotlib, and HierarchicalClustering from scikit-learn and scipy, for performing the clustering task. |\n",
        "| 2. Using the dendrogram to find the optimal number of clusters | We use the dendrogram to determine the optimal number of clusters for Hierarchical Clustering. The dendrogram visually represents the linkage distances between data points and helps us identify the appropriate number of clusters by observing the clustering structure. |\n",
        "| 3. Training the Hierarchical Clustering model    | After determining the optimal number of clusters from the dendrogram, we create an AgglomerativeClustering instance with that number of clusters and train it on the dataset using the fit_predict method. We use 'euclidean' distance and 'ward' linkage method to define how clusters are formed. |\n",
        "| 4. Visualizing the clusters                       | Finally, we plot the clusters on a 2D scatter plot to visualize how the data points have been grouped into different clusters. Each cluster is represented with a different color for clear distinction. |\n"
      ],
      "metadata": {
        "id": "5fBsP7-bjOFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV6w-dMhjKch"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Assuming you have your data in the variable X\n",
        "\n",
        "# Using the dendrogram to find the optimal number of clusters\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()\n",
        "\n",
        "# Training the Hierarchical Clustering model on the dataset with the optimal number of clusters\n",
        "optimal_clusters = # Fill in the optimal number of clusters obtained from the dendrogram\n",
        "hc = AgglomerativeClustering(n_clusters=optimal_clusters, affinity='euclidean', linkage='ward')\n",
        "y_hc = hc.fit_predict(X)\n",
        "\n",
        "# Visualizing the clusters\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s=100, c='red', label='Cluster 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s=100, c='blue', label='Cluster 2')\n",
        "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s=100, c='green', label='Cluster 3')\n",
        "plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s=100, c='cyan', label='Cluster 4')\n",
        "plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s=100, c='magenta', label='Cluster 5')\n",
        "plt.title('Clusters of customers')\n",
        "plt.xlabel('Annual Income (k$)')\n",
        "plt.ylabel('Spending Score (1-100)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}