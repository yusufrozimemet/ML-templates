{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1F1o4mx82D265Ifbm6S/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/reinforcement_learning/thompson_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                                       | Description                                                                                                                                           |\n",
        "|--------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Importing the libraries                   | Import the required libraries, including `random`, `numpy`, and `matplotlib.pyplot`. These libraries will be used for random number generation, mathematical calculations, and visualization. |\n",
        "| Importing the dataset                     | Import the dataset containing the rewards for each ad. The dataset will be used to simulate the multi-armed bandit problem, where each ad corresponds to an arm of the bandit. |\n",
        "| Implementing Thompson Sampling            | Initialize variables and lists to store the results of the Thompson Sampling algorithm. Set the number of rounds (N) and the number of ads (d). Then, iterate through each round (n) to select the ad based on the random draw from the Beta distribution. Update the reward counts and total reward based on the actual reward received. |\n",
        "| Visualising the results - Histogram      | Plot the histogram of ad selections to visualize the number of times each ad was selected during the N rounds of the experiment. This histogram provides insights into the effectiveness of the Thompson Sampling algorithm in selecting the best-performing ad. |\n"
      ],
      "metadata": {
        "id": "5fBsP7-bjOFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "\n",
        "# Importing the dataset (dataset is not provided in the code, assuming it's already loaded)\n",
        "\n",
        "# Thompson Sampling Algorithm\n",
        "N = 10000\n",
        "d = 10\n",
        "ads_selected = []\n",
        "numbers_of_rewards_1 = [0] * d\n",
        "numbers_of_rewards_0 = [0] * d\n",
        "total_reward = 0\n",
        "\n",
        "for n in range(0, N):\n",
        "    ad = 0\n",
        "    max_random = 0\n",
        "    for i in range(0, d):\n",
        "        random_beta = random.betavariate(numbers_of_rewards_1[i] + 1, numbers_of_rewards_0[i] + 1)\n",
        "        if random_beta > max_random:\n",
        "            max_random = random_beta\n",
        "            ad = i\n",
        "\n",
        "    ads_selected.append(ad)\n",
        "    reward = dataset.values[n, ad]  # Assuming dataset is a 2D array with rewards\n",
        "    if reward == 1:\n",
        "        numbers_of_rewards_1[ad] += 1\n",
        "    else:\n",
        "        numbers_of_rewards_0[ad] += 1\n",
        "    total_reward += reward\n",
        "\n",
        "# Visualising the results - Histogram\n"
      ],
      "metadata": {
        "id": "4llK0EaMherw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}