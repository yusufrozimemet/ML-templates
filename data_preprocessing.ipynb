{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8hEkw1bwfU0ZClav0Qlrx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                                    | Description                                                                                                          |\n",
        "|-----------------------------------------|----------------------------------------------------------------------------------------------------------------------|\n",
        "| **Step 1: Importing the libraries**     | Import the necessary Python libraries that will be used for data preprocessing, such as pandas, numpy, and scikit-learn modules.|\n",
        "| **Step 2: Importing the dataset**       | Load the dataset from a CSV file or other data sources into a pandas DataFrame for further processing.                |\n",
        "| **Step 3: Taking care of missing data** | Handle any missing values in the dataset. In this step, you can use techniques like SimpleImputer to replace missing values with mean, median, mode, or other strategies.|\n",
        "| **Step 4: Encoding categorical Independent Variable**   | Convert categorical variables into numerical format using OneHotEncoder, creating binary columns for each category. The original categorical columns are replaced with these binary columns. |\n",
        "| **Step 5: Encoding the Dependent Variable** | If the dependent variable (target) is categorical, use LabelEncoder to convert the categories into numerical representations.|\n",
        "| **Step 6: Splitting the dataset into Training set and Test set** | Split the dataset into training set and test set. The training set is used to train the machine learning model, while the test set is used to evaluate its performance. |\n",
        "| **Step 7: Feature Scaling**            | Scale the features to bring them to a similar range. Common techniques include Standardization and Min-Max Scaling.   |\n"
      ],
      "metadata": {
        "id": "AsrJUJFkt-_I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUnEezaSt89s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Step 1: Import the dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Step 2: Handle missing data\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "X[:, :] = imputer.fit_transform(X[:, :])\n",
        "\n",
        "# Step 3: Encode categorical data\n",
        "# Specify the column index/indices of categorical columns in X\n",
        "categorical_columns = [column_index1, column_index2, ...]\n",
        "ct = ColumnTransformer([(\"encoder\", OneHotEncoder(), categorical_columns)], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X), dtype=np.float)\n",
        "\n",
        "# Step 4: Encode the Dependent Variable (if applicable)\n",
        "label_encoder_y = LabelEncoder()\n",
        "y = label_encoder_y.fit_transform(y)\n",
        "\n",
        "# Step 5: Split the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Step 6: Feature Scaling (if necessary)\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n"
      ]
    }
  ]
}