{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCDVCeIpqUXk6und4qjzF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/classification/summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model                            | Description                                                         | Use Case                                 | Pros                                     | Cons                                        |\n",
        "|----------------------------------|---------------------------------------------------------------------|------------------------------------------|------------------------------------------|---------------------------------------------|\n",
        "| Decision Tree Classification     | A non-parametric algorithm that uses a tree-like model for          | - Binary and Multiclass Classification  | - Simple to understand and interpret.    | - Prone to overfitting with complex trees. |\n",
        "|                                  | classification tasks.                                                | - Both Numerical and Categorical Data    | - Non-linear relationships can be captured.| - Sensitive to small changes in data.      |\n",
        "| K-Nearest Neighbors (K-NN)       | A lazy learning algorithm that classifies samples based on         | - Binary Classification                 | - Intuitive and easy to implement.       | - Computationally expensive for large data.|\n",
        "|                                  | the majority class of its k-nearest neighbors in the feature space.  | - Real-Valued or Categorical Features    | - No training required.                  | - Sensitive to irrelevant features.        |\n",
        "| Kernel SVM                       | A powerful classifier that can handle non-linearly separable data   | - Binary and Multiclass Classification  | - Effective in high-dimensional spaces.  | - Computationally expensive for large data.|\n",
        "|                                  | by mapping it into a higher-dimensional feature space.              | - Complex Decision Boundaries            | - Effective for complex decision boundaries.| - Choice of kernel and hyperparameters.    |\n",
        "| Logistic Regression              | A linear classifier that models the probability of the class        | - Binary and Multiclass Classification  | - Simple and fast to train.             | - May underperform if relationships are non-linear.|\n",
        "|                                  | belonging to one of two or more possible outcomes.                  | - Real-Valued or Categorical Features    | - Easy to interpret coefficients.        | - Sensitive to outliers and noise.         |\n",
        "| Naive Bayes                      | A probabilistic classifier based on the Bayes theorem.              | - Text Classification                   | - Efficient and works well with high-dimensional data. | - Assumes independence between features.  |\n",
        "|                                  | It assumes independence between features given the class label.      | - Real-Valued or Categorical Features    | - Can handle categorical and numerical data. | - May not perform well with correlated features. |\n",
        "| Random Forest Classification     | An ensemble learning method that constructs multiple decision trees. | - Binary and Multiclass Classification  | - Reduces overfitting compared to individual trees. | - Less interpretable than single decision trees. |\n",
        "|                                  | It combines their outputs to make a final prediction.                | - Both Numerical and Categorical Data    | - Robust to outliers and noisy data.     | - Can be computationally expensive.        |\n",
        "| Support Vector Machine (SVM)     | A powerful classifier that finds the optimal hyperplane             | - Binary and Multiclass Classification  | - Effective in high-dimensional spaces.  | - Computationally expensive for large data.|\n",
        "|                                  | to maximize the margin between different classes.                   | - Linear and Non-linear Decision Boundaries | - Versatile with different kernel functions. | - Choice of kernel and hyperparameters.    |\n"
      ],
      "metadata": {
        "id": "AsrJUJFkt-_I"
      }
    }
  ]
}