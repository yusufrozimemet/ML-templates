{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFNMN1+jEqNN+7JWLkyHM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoseforaz0990/ML-templates/blob/main/regression/decision_tree_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                                                      | Description                                                                                                          |\n",
        "|-----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|\n",
        "| **Training the Decision Tree Regression model on the whole dataset** | Build a Decision Tree Regression model using the entire dataset.                                                     |\n",
        "|                                                           | The `DecisionTreeRegressor` class from scikit-learn is used to create the model.                                      |\n",
        "|                                                           | The `random_state` parameter is set to `0` for reproducibility in model training.                                     |\n",
        "|                                                           | Train the Decision Tree Regression model on the dataset to learn the relationships between the independent variable (Position level `X`) and the dependent variable (Salary `y`).   |\n",
        "| **Predicting a new result**                               | Predict the salary for a new position level (e.g., `6.5`) using the trained Decision Tree Regression model.       |\n",
        "|                                                           | The `predict()` method is used, passing the new position level as input to get the predicted salary.                   |\n",
        "| **Visualising the Decision Tree Regression results**      | Create a range of position levels (`X_grid`) from the minimum to the maximum value of the original position levels `X`. |\n",
        "|                                                           | The `DecisionTreeRegressor` model is then used to predict salaries for the position levels in `X_grid`.                |\n",
        "|                                                           | Create a scatter plot to visualize the actual salary (`y`) against the position level (`X`) in red data points.        |\n",
        "|                                                           | Plot the Decision Tree Regression predictions (salary) based on the `X_grid` in blue to visualize how well the model fits the data. |\n",
        "|                                                           | The blue curve represents the Decision Tree Regression predictions, capturing the relationship between position level and salary. |\n",
        "|                                                           | This visualization helps assess the performance of the Decision Tree Regression model and how well it captures the underlying patterns in the data. |\n",
        "| **Difference between Decision Tree Regression, SVR, SVC, and Polynomial Regression** |                                                                                                                  |\n",
        "|                                                           | **Decision Tree Regression:** It involves building a tree-like model to make predictions based on the independent variables. It splits the data into segments to create homogenous groups based on the dependent variable. Suitable for both linear and non-linear relationships between variables. |\n",
        "|                                                           |                                                                                                                      |\n",
        "|                                                           | **Support Vector Regression (SVR):** It is a non-linear regression technique that uses the Support Vector Machine (SVM) algorithm to model complex relationships between the dependent variable and the independent variables. It uses a kernel trick (e.g., RBF kernel) to map the data into a higher-dimensional space, allowing for non-linear modeling. SVR is suitable for cases where the relationship between variables is complex and cannot be captured well by linear models. |\n",
        "|                                                           |                                                                                                                      |\n",
        "|                                                           | **Support Vector Classification (SVC):** It is a classification technique using the SVM algorithm. SVC finds the best hyperplane that can separate two or more classes with a maximum margin. It is useful for binary and multi-class classification problems, particularly when the classes are not easily separable by a linear boundary. |\n",
        "|                                                           |                                                                                                                      |\n",
        "|                                                           | **Polynomial Regression:** It involves fitting a linear model to the transformed feature matrix `X_poly`, which includes polynomial combinations of the original features. This allows the model to capture nonlinear relationships between the dependent variable and the independent variables. Suitable for cases where data exhibits a curvilinear pattern.   |\n"
      ],
      "metadata": {
        "id": "AsrJUJFkt-_I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUnEezaSt89s"
      },
      "outputs": [],
      "source": [
        "# Training the Decision Tree Regression model on the whole dataset\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "regressor = DecisionTreeRegressor(random_state=0)\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# Predicting a new result\n",
        "new_position_level = 6.5\n",
        "predicted_salary = regressor.predict([[new_position_level]])\n",
        "\n",
        "# Visualising the Decision Tree Regression results\n",
        "import numpy as np\n",
        "X_grid = np.arange(min(X), max(X), 0.01)\n",
        "X_grid = X_grid.reshape((len(X_grid), 1))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X, y, color='red')\n",
        "plt.plot(X_grid, regressor.predict(X_grid), color='blue')\n",
        "plt.title('Truth or Bluff (Decision Tree Regression)')\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ]
}